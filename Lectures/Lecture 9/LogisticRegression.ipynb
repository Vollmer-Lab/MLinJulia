{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving the universe of linear models, we start to venture into generalized linear models (GLM). The first is\n",
    "**logistic regression** (also called binomial regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression behaves exactly like a linear model: it makes a prediction simply by computing a weighted\n",
    "sum of the independent variables $\\mathbf{X}$ by the estimated coefficients $\\boldsymbol{\\beta}$, plus an intercept\n",
    "$\\alpha$. However, instead of returning a continuous value $y$, such as linear regression, it returns the **logistic\n",
    "function** of $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Logistic}(x) = \\frac{1}{1 + e^{(-x)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use logistic regression when our dependent variable is binary. It has only two distinct values, usually\n",
    "encoded as $0$ or $1$. See the figure below for a graphical intuition of the logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip820\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip820)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip821\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip820)\" d=\"\n",
       "M249.542 1423.18 L2352.76 1423.18 L2352.76 47.2441 L249.542 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip822\">\n",
       "    <rect x=\"249\" y=\"47\" width=\"2104\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  309.067,1423.18 309.067,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  805.108,1423.18 805.108,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1301.15,1423.18 1301.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1797.19,1423.18 1797.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2293.23,1423.18 2293.23,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.067,1423.18 309.067,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  805.108,1423.18 805.108,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1301.15,1423.18 1301.15,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1797.19,1423.18 1797.19,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2293.23,1423.18 2293.23,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip820)\" d=\"M263.13 1468.75 L292.805 1468.75 L292.805 1472.69 L263.13 1472.69 L263.13 1468.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M303.708 1481.64 L311.347 1481.64 L311.347 1455.28 L303.037 1456.95 L303.037 1452.69 L311.301 1451.02 L315.977 1451.02 L315.977 1481.64 L323.615 1481.64 L323.615 1485.58 L303.708 1485.58 L303.708 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M343.06 1454.1 Q339.449 1454.1 337.62 1457.66 Q335.814 1461.2 335.814 1468.33 Q335.814 1475.44 337.62 1479.01 Q339.449 1482.55 343.06 1482.55 Q346.694 1482.55 348.5 1479.01 Q350.328 1475.44 350.328 1468.33 Q350.328 1461.2 348.5 1457.66 Q346.694 1454.1 343.06 1454.1 M343.06 1450.39 Q348.87 1450.39 351.926 1455 Q355.004 1459.58 355.004 1468.33 Q355.004 1477.06 351.926 1481.67 Q348.87 1486.25 343.06 1486.25 Q337.25 1486.25 334.171 1481.67 Q331.115 1477.06 331.115 1468.33 Q331.115 1459.58 334.171 1455 Q337.25 1450.39 343.06 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M774.749 1468.75 L804.425 1468.75 L804.425 1472.69 L774.749 1472.69 L774.749 1468.75 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M814.564 1451.02 L832.92 1451.02 L832.92 1454.96 L818.846 1454.96 L818.846 1463.43 Q819.865 1463.08 820.883 1462.92 Q821.902 1462.73 822.92 1462.73 Q828.707 1462.73 832.087 1465.9 Q835.467 1469.08 835.467 1474.49 Q835.467 1480.07 831.994 1483.17 Q828.522 1486.25 822.203 1486.25 Q820.027 1486.25 817.758 1485.88 Q815.513 1485.51 813.106 1484.77 L813.106 1480.07 Q815.189 1481.2 817.411 1481.76 Q819.633 1482.32 822.11 1482.32 Q826.115 1482.32 828.453 1480.21 Q830.791 1478.1 830.791 1474.49 Q830.791 1470.88 828.453 1468.77 Q826.115 1466.67 822.11 1466.67 Q820.235 1466.67 818.36 1467.08 Q816.508 1467.5 814.564 1468.38 L814.564 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M1301.15 1454.1 Q1297.54 1454.1 1295.71 1457.66 Q1293.9 1461.2 1293.9 1468.33 Q1293.9 1475.44 1295.71 1479.01 Q1297.54 1482.55 1301.15 1482.55 Q1304.78 1482.55 1306.59 1479.01 Q1308.42 1475.44 1308.42 1468.33 Q1308.42 1461.2 1306.59 1457.66 Q1304.78 1454.1 1301.15 1454.1 M1301.15 1450.39 Q1306.96 1450.39 1310.01 1455 Q1313.09 1459.58 1313.09 1468.33 Q1313.09 1477.06 1310.01 1481.67 Q1306.96 1486.25 1301.15 1486.25 Q1295.34 1486.25 1292.26 1481.67 Q1289.2 1477.06 1289.2 1468.33 Q1289.2 1459.58 1292.26 1455 Q1295.34 1450.39 1301.15 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M1787.47 1451.02 L1805.82 1451.02 L1805.82 1454.96 L1791.75 1454.96 L1791.75 1463.43 Q1792.77 1463.08 1793.79 1462.92 Q1794.81 1462.73 1795.82 1462.73 Q1801.61 1462.73 1804.99 1465.9 Q1808.37 1469.08 1808.37 1474.49 Q1808.37 1480.07 1804.9 1483.17 Q1801.43 1486.25 1795.11 1486.25 Q1792.93 1486.25 1790.66 1485.88 Q1788.42 1485.51 1786.01 1484.77 L1786.01 1480.07 Q1788.09 1481.2 1790.32 1481.76 Q1792.54 1482.32 1795.01 1482.32 Q1799.02 1482.32 1801.36 1480.21 Q1803.69 1478.1 1803.69 1474.49 Q1803.69 1470.88 1801.36 1468.77 Q1799.02 1466.67 1795.01 1466.67 Q1793.14 1466.67 1791.26 1467.08 Q1789.41 1467.5 1787.47 1468.38 L1787.47 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M2267.92 1481.64 L2275.56 1481.64 L2275.56 1455.28 L2267.25 1456.95 L2267.25 1452.69 L2275.51 1451.02 L2280.19 1451.02 L2280.19 1481.64 L2287.83 1481.64 L2287.83 1485.58 L2267.92 1485.58 L2267.92 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M2307.27 1454.1 Q2303.66 1454.1 2301.83 1457.66 Q2300.02 1461.2 2300.02 1468.33 Q2300.02 1475.44 2301.83 1479.01 Q2303.66 1482.55 2307.27 1482.55 Q2310.9 1482.55 2312.71 1479.01 Q2314.54 1475.44 2314.54 1468.33 Q2314.54 1461.2 2312.71 1457.66 Q2310.9 1454.1 2307.27 1454.1 M2307.27 1450.39 Q2313.08 1450.39 2316.14 1455 Q2319.21 1459.58 2319.21 1468.33 Q2319.21 1477.06 2316.14 1481.67 Q2313.08 1486.25 2307.27 1486.25 Q2301.46 1486.25 2298.38 1481.67 Q2295.33 1477.06 2295.33 1468.33 Q2295.33 1459.58 2298.38 1455 Q2301.46 1450.39 2307.27 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M1315.32 1528.8 Q1315.32 1530.61 1314.2 1531.64 Q1313.07 1532.64 1311.78 1532.64 Q1310.56 1532.64 1309.91 1531.93 Q1309.27 1531.22 1309.27 1530.32 Q1309.27 1529.09 1310.17 1528.03 Q1311.07 1526.97 1312.42 1526.74 Q1311.1 1525.91 1309.14 1525.91 Q1307.85 1525.91 1306.72 1526.58 Q1305.63 1527.26 1304.95 1528.13 Q1304.31 1529 1303.73 1530.25 Q1303.18 1531.48 1302.96 1532.22 Q1302.76 1532.93 1302.6 1533.7 L1300.35 1542.72 Q1299.25 1547 1299.25 1548.51 Q1299.25 1550.38 1300.15 1551.64 Q1301.06 1552.86 1302.86 1552.86 Q1303.57 1552.86 1304.37 1552.67 Q1305.18 1552.44 1306.21 1551.86 Q1307.27 1551.25 1308.2 1550.35 Q1309.17 1549.42 1310.11 1547.84 Q1311.04 1546.26 1311.65 1544.23 Q1311.84 1543.52 1312.49 1543.52 Q1313.29 1543.52 1313.29 1544.17 Q1313.29 1544.71 1312.84 1545.87 Q1312.42 1547 1311.49 1548.48 Q1310.59 1549.93 1309.4 1551.25 Q1308.2 1552.54 1306.43 1553.44 Q1304.66 1554.34 1302.73 1554.34 Q1299.96 1554.34 1298.12 1552.86 Q1296.29 1551.38 1295.61 1549.32 Q1295.45 1549.61 1295.23 1550 Q1295 1550.38 1294.32 1551.25 Q1293.68 1552.09 1292.94 1552.73 Q1292.2 1553.34 1291.04 1553.83 Q1289.91 1554.34 1288.69 1554.34 Q1287.14 1554.34 1285.76 1553.89 Q1284.4 1553.44 1283.44 1552.41 Q1282.47 1551.38 1282.47 1549.96 Q1282.47 1548.38 1283.54 1547.29 Q1284.63 1546.16 1286.11 1546.16 Q1287.05 1546.16 1287.79 1546.71 Q1288.56 1547.26 1288.56 1548.45 Q1288.56 1549.77 1287.66 1550.77 Q1286.76 1551.77 1285.47 1552.02 Q1286.79 1552.86 1288.75 1552.86 Q1290.88 1552.86 1292.55 1550.99 Q1294.23 1549.13 1295.03 1546 Q1297.03 1538.5 1297.8 1535.15 Q1298.58 1531.77 1298.58 1530.32 Q1298.58 1528.96 1298.22 1528.03 Q1297.87 1527.1 1297.25 1526.68 Q1296.68 1526.23 1296.13 1526.07 Q1295.61 1525.91 1295.03 1525.91 Q1294.07 1525.91 1292.97 1526.29 Q1291.91 1526.68 1290.62 1527.58 Q1289.36 1528.45 1288.17 1530.25 Q1286.98 1532.06 1286.18 1534.54 Q1286.02 1535.28 1285.31 1535.28 Q1284.53 1535.24 1284.53 1534.6 Q1284.53 1534.05 1284.95 1532.93 Q1285.4 1531.77 1286.3 1530.32 Q1287.24 1528.87 1288.43 1527.58 Q1289.65 1526.26 1291.43 1525.36 Q1293.23 1524.46 1295.16 1524.46 Q1296.03 1524.46 1296.87 1524.65 Q1297.74 1524.81 1298.77 1525.29 Q1299.83 1525.78 1300.77 1526.84 Q1301.7 1527.9 1302.28 1529.45 Q1302.67 1528.71 1303.18 1528 Q1303.73 1527.29 1304.57 1526.42 Q1305.44 1525.52 1306.63 1525 Q1307.85 1524.46 1309.2 1524.46 Q1310.52 1524.46 1311.81 1524.81 Q1313.1 1525.13 1314.2 1526.19 Q1315.32 1527.23 1315.32 1528.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,1384.3 2352.76,1384.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,1059.75 2352.76,1059.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,735.212 2352.76,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,410.669 2352.76,410.669 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip822)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.542,86.1267 2352.76,86.1267 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1423.18 249.542,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1384.3 268.44,1384.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,1059.75 268.44,1059.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,735.212 268.44,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,410.669 268.44,410.669 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip820)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.542,86.1267 268.44,86.1267 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip820)\" d=\"M126.205 1370.1 Q122.593 1370.1 120.765 1373.66 Q118.959 1377.2 118.959 1384.33 Q118.959 1391.44 120.765 1395 Q122.593 1398.54 126.205 1398.54 Q129.839 1398.54 131.644 1395 Q133.473 1391.44 133.473 1384.33 Q133.473 1377.2 131.644 1373.66 Q129.839 1370.1 126.205 1370.1 M126.205 1366.39 Q132.015 1366.39 135.07 1371 Q138.149 1375.58 138.149 1384.33 Q138.149 1393.06 135.07 1397.67 Q132.015 1402.25 126.205 1402.25 Q120.394 1402.25 117.316 1397.67 Q114.26 1393.06 114.26 1384.33 Q114.26 1375.58 117.316 1371 Q120.394 1366.39 126.205 1366.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M146.366 1395.7 L151.251 1395.7 L151.251 1401.58 L146.366 1401.58 L146.366 1395.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M171.436 1370.1 Q167.825 1370.1 165.996 1373.66 Q164.19 1377.2 164.19 1384.33 Q164.19 1391.44 165.996 1395 Q167.825 1398.54 171.436 1398.54 Q175.07 1398.54 176.876 1395 Q178.704 1391.44 178.704 1384.33 Q178.704 1377.2 176.876 1373.66 Q175.07 1370.1 171.436 1370.1 M171.436 1366.39 Q177.246 1366.39 180.301 1371 Q183.38 1375.58 183.38 1384.33 Q183.38 1393.06 180.301 1397.67 Q177.246 1402.25 171.436 1402.25 Q165.626 1402.25 162.547 1397.67 Q159.491 1393.06 159.491 1384.33 Q159.491 1375.58 162.547 1371 Q165.626 1366.39 171.436 1366.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M201.598 1370.1 Q197.987 1370.1 196.158 1373.66 Q194.352 1377.2 194.352 1384.33 Q194.352 1391.44 196.158 1395 Q197.987 1398.54 201.598 1398.54 Q205.232 1398.54 207.037 1395 Q208.866 1391.44 208.866 1384.33 Q208.866 1377.2 207.037 1373.66 Q205.232 1370.1 201.598 1370.1 M201.598 1366.39 Q207.408 1366.39 210.463 1371 Q213.542 1375.58 213.542 1384.33 Q213.542 1393.06 210.463 1397.67 Q207.408 1402.25 201.598 1402.25 Q195.787 1402.25 192.709 1397.67 Q189.653 1393.06 189.653 1384.33 Q189.653 1375.58 192.709 1371 Q195.787 1366.39 201.598 1366.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M127.2 1045.55 Q123.589 1045.55 121.76 1049.12 Q119.955 1052.66 119.955 1059.79 Q119.955 1066.9 121.76 1070.46 Q123.589 1074 127.2 1074 Q130.834 1074 132.64 1070.46 Q134.468 1066.9 134.468 1059.79 Q134.468 1052.66 132.64 1049.12 Q130.834 1045.55 127.2 1045.55 M127.2 1041.85 Q133.01 1041.85 136.066 1046.46 Q139.144 1051.04 139.144 1059.79 Q139.144 1068.52 136.066 1073.12 Q133.01 1077.71 127.2 1077.71 Q121.39 1077.71 118.311 1073.12 Q115.256 1068.52 115.256 1059.79 Q115.256 1051.04 118.311 1046.46 Q121.39 1041.85 127.2 1041.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M147.362 1071.15 L152.246 1071.15 L152.246 1077.03 L147.362 1077.03 L147.362 1071.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M166.459 1073.1 L182.778 1073.1 L182.778 1077.03 L160.834 1077.03 L160.834 1073.1 Q163.496 1070.34 168.079 1065.72 Q172.686 1061.06 173.866 1059.72 Q176.112 1057.2 176.991 1055.46 Q177.894 1053.7 177.894 1052.01 Q177.894 1049.26 175.95 1047.52 Q174.028 1045.78 170.927 1045.78 Q168.727 1045.78 166.274 1046.55 Q163.843 1047.31 161.065 1048.86 L161.065 1044.14 Q163.89 1043.01 166.343 1042.43 Q168.797 1041.85 170.834 1041.85 Q176.204 1041.85 179.399 1044.53 Q182.593 1047.22 182.593 1051.71 Q182.593 1053.84 181.783 1055.76 Q180.996 1057.66 178.889 1060.25 Q178.311 1060.92 175.209 1064.14 Q172.107 1067.34 166.459 1073.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M192.639 1042.47 L210.996 1042.47 L210.996 1046.41 L196.922 1046.41 L196.922 1054.88 Q197.94 1054.53 198.959 1054.37 Q199.977 1054.19 200.996 1054.19 Q206.783 1054.19 210.162 1057.36 Q213.542 1060.53 213.542 1065.95 Q213.542 1071.53 210.07 1074.63 Q206.598 1077.71 200.278 1077.71 Q198.102 1077.71 195.834 1077.34 Q193.588 1076.97 191.181 1076.22 L191.181 1071.53 Q193.264 1072.66 195.487 1073.22 Q197.709 1073.77 200.186 1073.77 Q204.19 1073.77 206.528 1071.66 Q208.866 1069.56 208.866 1065.95 Q208.866 1062.34 206.528 1060.23 Q204.19 1058.12 200.186 1058.12 Q198.311 1058.12 196.436 1058.54 Q194.584 1058.96 192.639 1059.84 L192.639 1042.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M126.205 721.011 Q122.593 721.011 120.765 724.575 Q118.959 728.117 118.959 735.247 Q118.959 742.353 120.765 745.918 Q122.593 749.46 126.205 749.46 Q129.839 749.46 131.644 745.918 Q133.473 742.353 133.473 735.247 Q133.473 728.117 131.644 724.575 Q129.839 721.011 126.205 721.011 M126.205 717.307 Q132.015 717.307 135.07 721.913 Q138.149 726.497 138.149 735.247 Q138.149 743.973 135.07 748.58 Q132.015 753.163 126.205 753.163 Q120.394 753.163 117.316 748.58 Q114.26 743.973 114.26 735.247 Q114.26 726.497 117.316 721.913 Q120.394 717.307 126.205 717.307 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M146.366 746.612 L151.251 746.612 L151.251 752.492 L146.366 752.492 L146.366 746.612 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M161.482 717.932 L179.839 717.932 L179.839 721.867 L165.765 721.867 L165.765 730.339 Q166.783 729.992 167.802 729.83 Q168.82 729.645 169.839 729.645 Q175.626 729.645 179.005 732.816 Q182.385 735.987 182.385 741.404 Q182.385 746.983 178.913 750.085 Q175.44 753.163 169.121 753.163 Q166.945 753.163 164.677 752.793 Q162.431 752.423 160.024 751.682 L160.024 746.983 Q162.107 748.117 164.329 748.673 Q166.552 749.228 169.028 749.228 Q173.033 749.228 175.371 747.122 Q177.709 745.015 177.709 741.404 Q177.709 737.793 175.371 735.687 Q173.033 733.58 169.028 733.58 Q167.153 733.58 165.278 733.997 Q163.427 734.413 161.482 735.293 L161.482 717.932 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M201.598 721.011 Q197.987 721.011 196.158 724.575 Q194.352 728.117 194.352 735.247 Q194.352 742.353 196.158 745.918 Q197.987 749.46 201.598 749.46 Q205.232 749.46 207.037 745.918 Q208.866 742.353 208.866 735.247 Q208.866 728.117 207.037 724.575 Q205.232 721.011 201.598 721.011 M201.598 717.307 Q207.408 717.307 210.463 721.913 Q213.542 726.497 213.542 735.247 Q213.542 743.973 210.463 748.58 Q207.408 753.163 201.598 753.163 Q195.787 753.163 192.709 748.58 Q189.653 743.973 189.653 735.247 Q189.653 726.497 192.709 721.913 Q195.787 717.307 201.598 717.307 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M127.2 396.468 Q123.589 396.468 121.76 400.033 Q119.955 403.574 119.955 410.704 Q119.955 417.811 121.76 421.375 Q123.589 424.917 127.2 424.917 Q130.834 424.917 132.64 421.375 Q134.468 417.811 134.468 410.704 Q134.468 403.574 132.64 400.033 Q130.834 396.468 127.2 396.468 M127.2 392.764 Q133.01 392.764 136.066 397.371 Q139.144 401.954 139.144 410.704 Q139.144 419.431 136.066 424.037 Q133.01 428.621 127.2 428.621 Q121.39 428.621 118.311 424.037 Q115.256 419.431 115.256 410.704 Q115.256 401.954 118.311 397.371 Q121.39 392.764 127.2 392.764 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M147.362 422.07 L152.246 422.07 L152.246 427.949 L147.362 427.949 L147.362 422.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M161.251 393.389 L183.473 393.389 L183.473 395.38 L170.927 427.949 L166.042 427.949 L177.848 397.325 L161.251 397.325 L161.251 393.389 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M192.639 393.389 L210.996 393.389 L210.996 397.325 L196.922 397.325 L196.922 405.797 Q197.94 405.449 198.959 405.287 Q199.977 405.102 200.996 405.102 Q206.783 405.102 210.162 408.274 Q213.542 411.445 213.542 416.861 Q213.542 422.44 210.07 425.542 Q206.598 428.621 200.278 428.621 Q198.102 428.621 195.834 428.25 Q193.588 427.88 191.181 427.139 L191.181 422.44 Q193.264 423.574 195.487 424.13 Q197.709 424.685 200.186 424.685 Q204.19 424.685 206.528 422.579 Q208.866 420.473 208.866 416.861 Q208.866 413.25 206.528 411.144 Q204.19 409.037 200.186 409.037 Q198.311 409.037 196.436 409.454 Q194.584 409.871 192.639 410.75 L192.639 393.389 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M117.015 99.4716 L124.654 99.4716 L124.654 73.106 L116.343 74.7726 L116.343 70.5134 L124.607 68.8467 L129.283 68.8467 L129.283 99.4716 L136.922 99.4716 L136.922 103.407 L117.015 103.407 L117.015 99.4716 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M146.366 97.5271 L151.251 97.5271 L151.251 103.407 L146.366 103.407 L146.366 97.5271 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M171.436 71.9254 Q167.825 71.9254 165.996 75.4902 Q164.19 79.0319 164.19 86.1615 Q164.19 93.2679 165.996 96.8327 Q167.825 100.374 171.436 100.374 Q175.07 100.374 176.876 96.8327 Q178.704 93.2679 178.704 86.1615 Q178.704 79.0319 176.876 75.4902 Q175.07 71.9254 171.436 71.9254 M171.436 68.2217 Q177.246 68.2217 180.301 72.8282 Q183.38 77.4115 183.38 86.1615 Q183.38 94.8883 180.301 99.4947 Q177.246 104.078 171.436 104.078 Q165.626 104.078 162.547 99.4947 Q159.491 94.8883 159.491 86.1615 Q159.491 77.4115 162.547 72.8282 Q165.626 68.2217 171.436 68.2217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M201.598 71.9254 Q197.987 71.9254 196.158 75.4902 Q194.352 79.0319 194.352 86.1615 Q194.352 93.2679 196.158 96.8327 Q197.987 100.374 201.598 100.374 Q205.232 100.374 207.037 96.8327 Q208.866 93.2679 208.866 86.1615 Q208.866 79.0319 207.037 75.4902 Q205.232 71.9254 201.598 71.9254 M201.598 68.2217 Q207.408 68.2217 210.463 72.8282 Q213.542 77.4115 213.542 86.1615 Q213.542 94.8883 210.463 99.4947 Q207.408 104.078 201.598 104.078 Q195.787 104.078 192.709 99.4947 Q189.653 94.8883 189.653 86.1615 Q189.653 77.4115 192.709 72.8282 Q195.787 68.2217 201.598 68.2217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M66.0841 889.363 L64.0551 889.363 L64.0551 887.785 Q64.0551 884.21 63.4754 883.405 Q62.8635 882.568 60.9312 882.568 L26.181 882.568 Q24.2809 882.568 23.7012 883.405 Q23.0893 884.242 23.0893 887.785 L23.0893 889.363 L21.0281 889.363 Q21.2213 887.044 21.2213 879.347 Q21.2213 870.652 21.0281 868.268 L23.0893 868.268 L23.0893 870.458 Q23.0893 873.486 23.4757 874.871 Q23.8622 876.255 24.4097 876.481 Q24.925 876.706 26.2454 876.706 L61.4142 876.706 Q63.1212 876.706 63.6042 876.223 Q64.0551 875.708 64.0551 873.486 L64.0551 868.075 Q64.0551 864.726 63.0567 862.31 Q62.0584 859.895 60.8023 858.607 Q59.5463 857.318 57.1953 856.481 Q54.812 855.611 53.2984 855.354 Q51.7847 855.064 49.0794 854.806 L49.0794 853.164 L66.0841 854.999 L66.0841 889.363 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M62.5415 844.185 Q58.2259 848.533 51.9779 848.533 Q45.6978 848.533 41.1246 844.282 Q36.5191 839.999 36.5191 833.879 Q36.5191 827.889 41.0923 823.606 Q45.6334 819.29 51.9779 819.29 Q58.1937 819.29 62.5092 823.638 Q66.8248 827.954 66.8248 833.944 Q66.8248 839.805 62.5415 844.185 M51.4304 843.058 Q57.8394 843.058 60.7379 841.383 Q65.1501 838.807 65.1501 833.879 Q65.1501 831.432 63.8297 829.403 Q62.5092 827.342 60.287 826.214 Q57.3885 824.765 51.4304 824.765 Q45.0859 824.765 42.284 826.504 Q38.0006 829.081 38.0006 833.944 Q38.0006 836.069 39.1278 838.163 Q40.2228 840.224 42.4128 841.48 Q45.3113 843.058 51.4304 843.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M71.3014 818.245 Q69.1759 818.245 67.3401 816.635 Q65.5044 814.992 64.6992 812.158 Q62.8635 815.057 58.8378 815.057 Q55.746 815.057 53.3628 813.028 Q50.7863 816.119 46.5673 816.119 Q42.6704 816.119 39.8041 813.028 Q36.9378 809.936 36.9378 805.427 Q36.9378 801.466 39.321 798.438 Q36.2293 795.282 36.1971 791.45 Q36.1971 789.743 37.2598 788.938 Q38.3226 788.1 39.4498 788.1 Q40.4482 788.1 40.9313 788.744 Q41.4144 789.356 41.4144 790 Q41.4144 790.806 40.8991 791.385 Q40.3516 791.933 39.5143 791.933 Q38.1294 791.933 37.7107 790.87 Q37.6463 791.063 37.6463 791.514 Q37.6463 794.735 40.2228 797.472 Q42.7348 794.767 46.6318 794.767 Q50.5287 794.767 53.395 797.859 Q56.2613 800.95 56.2613 805.427 Q56.2613 809.131 54.2001 811.965 Q55.5206 813.092 57.3241 813.092 Q58.9666 813.092 60.2548 812.094 Q61.5431 811.095 61.7363 809.582 Q61.8007 809.131 61.8007 804.654 Q61.8007 802.013 61.8651 800.564 Q61.9295 799.115 62.3804 797.054 Q62.7991 794.96 63.6364 793.318 Q66.0197 789.034 71.1726 789.002 Q74.9407 789.002 77.2917 793.35 Q79.675 797.665 79.675 803.656 Q79.675 809.71 77.2595 813.994 Q74.8763 818.245 71.3014 818.245 M71.3014 814.799 Q74.0067 814.799 76.0679 811.611 Q78.1613 808.422 78.1613 803.591 Q78.1613 798.857 76.1001 795.669 Q74.0711 792.448 71.3014 792.448 Q69.3369 792.448 68.0809 793.575 Q66.8248 794.703 66.3095 797.021 Q65.8264 799.308 65.7298 800.918 Q65.6332 802.529 65.6332 805.491 L65.6332 809.388 Q65.762 811.643 67.4045 813.221 Q69.047 814.799 71.3014 814.799 M46.6318 811.192 Q54.7476 811.192 54.7476 805.427 Q54.7476 802.529 52.1068 800.757 Q50.3354 799.694 46.5673 799.694 Q38.4515 799.694 38.4515 805.427 Q38.4515 808.326 41.0923 810.129 Q42.8637 811.192 46.6318 811.192 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M66.0841 782.677 L64.0551 782.677 Q64.0551 779.167 63.6364 778.361 Q63.1856 777.556 61.06 777.556 L43.3146 777.556 Q40.8669 777.556 40.2872 778.394 Q39.7075 779.199 39.7075 782.419 L37.6463 782.419 L36.9378 773.208 L61.1244 773.208 Q63.0889 773.208 63.572 772.5 Q64.0551 771.791 64.0551 768.571 L66.0841 768.571 Q65.8909 775.302 65.8909 775.431 Q65.8909 776.332 66.0841 782.677 M25.4403 779.907 Q24.1199 779.907 23.0571 778.909 Q21.9621 777.878 21.9621 776.429 Q21.9621 774.98 22.9604 773.949 Q23.9266 772.919 25.4725 772.919 Q26.9862 772.919 27.9846 773.949 Q28.9507 774.98 28.9507 776.429 Q28.9507 777.943 27.8879 778.941 Q26.8252 779.907 25.4403 779.907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M65.2145 762.743 L56.519 762.743 Q55.7782 762.743 55.5206 762.71 Q55.2629 762.678 55.0697 762.485 Q54.8765 762.292 54.8765 761.905 Q54.8765 761.454 55.0697 761.293 Q55.2629 761.1 56.0037 760.971 Q60.5447 759.973 62.9601 757.847 Q65.3434 755.689 65.3434 751.793 Q65.3434 748.089 63.7009 746.221 Q62.0584 744.353 59.3531 744.353 Q54.5222 744.353 53.2984 751.213 Q52.4932 755.174 51.9779 756.817 Q51.4304 758.459 50.2388 759.908 Q47.92 762.743 44.635 762.743 Q41.35 762.743 38.9346 760.263 Q36.5191 757.751 36.5191 752.211 Q36.5191 748.508 38.3871 745.995 Q37.8073 745.255 37.3243 744.868 Q36.5191 743.999 36.5191 743.548 Q36.5191 743.033 36.8734 742.936 Q37.1954 742.839 38.1294 742.839 L44.7638 742.839 Q45.5045 742.839 45.7622 742.871 Q46.0198 742.904 46.2131 743.097 Q46.3741 743.29 46.3741 743.709 Q46.3741 744.45 45.7622 744.482 Q37.7751 744.997 37.7751 752.211 Q37.7751 756.108 39.2888 757.847 Q40.7703 759.586 42.7993 759.586 Q43.9265 759.586 44.796 759.071 Q45.6334 758.524 46.1487 757.847 Q46.6318 757.139 47.1148 755.85 Q47.5657 754.562 47.7268 753.789 Q47.8878 752.984 48.2098 751.47 Q49.2082 746.189 51.366 743.966 Q54.1357 741.197 57.6462 741.197 Q61.5431 741.197 64.1839 743.838 Q66.8248 746.479 66.8248 751.793 Q66.8248 756.076 63.9585 759.071 Q64.345 759.458 64.6992 759.747 Q65.0213 760.037 65.1501 760.166 Q65.2789 760.263 65.3756 760.295 Q65.44 760.327 65.5044 760.391 Q66.8248 761.712 66.8248 762.034 Q66.8248 762.549 66.4706 762.646 Q66.1485 762.743 65.2145 762.743 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M39.7075 739.366 L38.2582 739.366 Q38.1294 736.339 36.0038 734.342 Q33.846 732.313 31.2051 731.637 Q28.5643 730.928 25.5047 730.864 L25.5047 729.221 L37.6463 729.221 L37.6463 719.785 L39.7075 719.785 L39.7075 729.221 L58.0326 729.221 Q65.1501 729.221 65.1501 724.777 Q65.1501 722.877 63.2178 721.621 Q61.2532 720.365 57.775 720.365 L54.1357 720.365 L54.1357 718.722 L57.9038 718.722 Q61.4787 718.722 64.1517 720.365 Q66.8248 722.007 66.8248 725.26 Q66.8248 726.452 66.5028 727.643 Q66.2129 728.803 65.44 730.349 Q64.6348 731.862 62.7025 732.829 Q60.7379 733.762 57.9038 733.762 L39.7075 733.762 L39.7075 739.366 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M66.0841 714.831 L64.0551 714.831 Q64.0551 711.32 63.6364 710.515 Q63.1856 709.71 61.06 709.71 L43.3146 709.71 Q40.8669 709.71 40.2872 710.548 Q39.7075 711.353 39.7075 714.573 L37.6463 714.573 L36.9378 705.362 L61.1244 705.362 Q63.0889 705.362 63.572 704.654 Q64.0551 703.945 64.0551 700.725 L66.0841 700.725 Q65.8909 707.456 65.8909 707.585 Q65.8909 708.486 66.0841 714.831 M25.4403 712.061 Q24.1199 712.061 23.0571 711.063 Q21.9621 710.032 21.9621 708.583 Q21.9621 707.134 22.9604 706.103 Q23.9266 705.073 25.4725 705.073 Q26.9862 705.073 27.9846 706.103 Q28.9507 707.134 28.9507 708.583 Q28.9507 710.097 27.8879 711.095 Q26.8252 712.061 25.4403 712.061 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M62.477 690.71 Q58.097 694.896 51.8491 694.896 Q45.569 694.896 41.0601 690.774 Q36.5191 686.652 36.5191 680.597 Q36.5191 676.571 38.4515 673.544 Q40.3516 670.517 43.6044 670.517 Q45.0537 670.517 45.891 671.386 Q46.6962 672.224 46.6962 673.544 Q46.6962 674.929 45.8588 675.766 Q44.9893 676.571 43.6688 676.571 Q43.0891 676.571 42.5094 676.378 Q41.9297 676.185 41.3178 675.476 Q40.7059 674.736 40.6093 673.48 Q38.1938 675.831 38.1938 680.404 Q38.1938 680.468 38.1938 680.533 Q38.1938 683.882 41.35 686.652 Q44.5062 689.421 51.7203 689.421 Q55.4884 689.421 58.2581 688.52 Q60.9956 687.586 62.4126 686.072 Q63.8297 684.558 64.506 683.077 Q65.1501 681.595 65.1501 680.146 Q65.1501 673.673 58.2259 671.483 Q57.5817 671.29 57.5817 670.645 Q57.5817 669.776 58.2259 669.776 Q58.5479 669.776 59.3853 670.034 Q60.2226 670.291 61.5431 671.096 Q62.8635 671.901 64.0229 673.061 Q65.1501 674.188 65.9875 676.217 Q66.8248 678.214 66.8248 680.726 Q66.8248 686.523 62.477 690.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M49.5947 665.901 Q38.4515 665.901 29.466 661.746 Q25.6979 659.975 22.5418 657.495 Q19.3856 655.015 18.0007 653.373 Q16.6159 651.73 16.6159 651.279 Q16.6159 650.635 17.26 650.603 Q17.5821 650.603 18.3872 651.473 Q29.2084 662.1 49.5947 662.068 Q70.0454 662.068 80.4479 651.73 Q81.5751 650.603 81.9294 650.603 Q82.5735 650.603 82.5735 651.279 Q82.5735 651.73 81.2531 653.308 Q79.9326 654.886 76.9053 657.334 Q73.8779 659.782 70.1742 661.553 Q61.1888 665.901 49.5947 665.901 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M41.2856 612.739 Q43.0891 612.739 44.1197 613.867 Q45.1181 614.994 45.1181 616.282 Q45.1181 617.506 44.4096 618.15 Q43.701 618.794 42.7993 618.794 Q41.5754 618.794 40.5126 617.892 Q39.4498 616.991 39.2244 615.638 Q38.3871 616.958 38.3871 618.923 Q38.3871 620.211 39.0634 621.338 Q39.7397 622.433 40.6093 623.11 Q41.4788 623.754 42.7348 624.333 Q43.9587 624.881 44.6994 625.106 Q45.4079 625.3 46.1809 625.461 L55.1985 627.715 Q59.4819 628.81 60.9956 628.81 Q62.8635 628.81 64.1195 627.908 Q65.3434 627.007 65.3434 625.203 Q65.3434 624.494 65.1501 623.689 Q64.9247 622.884 64.345 621.854 Q63.7331 620.791 62.8313 619.857 Q61.8973 618.891 60.3192 617.957 Q58.7412 617.023 56.7122 616.411 Q56.0037 616.218 56.0037 615.573 Q56.0037 614.768 56.6478 614.768 Q57.1953 614.768 58.3547 615.219 Q59.4819 615.638 60.9634 616.572 Q62.4126 617.474 63.7331 618.665 Q65.0213 619.857 65.9231 621.628 Q66.8248 623.399 66.8248 625.332 Q66.8248 628.102 65.3434 629.937 Q63.8619 631.773 61.8007 632.449 Q62.0906 632.61 62.477 632.836 Q62.8635 633.061 63.7331 633.738 Q64.5704 634.382 65.2145 635.122 Q65.8264 635.863 66.3095 637.023 Q66.8248 638.15 66.8248 639.374 Q66.8248 640.919 66.3739 642.304 Q65.9231 643.657 64.8925 644.623 Q63.8619 645.589 62.4448 645.589 Q60.8667 645.589 59.7717 644.527 Q58.6445 643.432 58.6445 641.95 Q58.6445 641.016 59.192 640.275 Q59.7395 639.502 60.9312 639.502 Q62.2516 639.502 63.25 640.404 Q64.2484 641.306 64.506 642.594 Q65.3434 641.274 65.3434 639.309 Q65.3434 637.184 63.4754 635.509 Q61.6075 633.834 58.4835 633.029 Q50.9795 631.032 47.6301 630.259 Q44.2485 629.486 42.7993 629.486 Q41.4466 629.486 40.5126 629.841 Q39.5787 630.195 39.16 630.807 Q38.7091 631.387 38.5481 631.934 Q38.3871 632.449 38.3871 633.029 Q38.3871 633.995 38.7735 635.09 Q39.16 636.153 40.0618 637.441 Q40.9313 638.697 42.7348 639.889 Q44.5384 641.081 47.0182 641.886 Q47.759 642.047 47.759 642.755 Q47.7268 643.528 47.0826 643.528 Q46.5351 643.528 45.4079 643.109 Q44.2485 642.659 42.7993 641.757 Q41.35 640.823 40.0618 639.631 Q38.7413 638.407 37.8396 636.636 Q36.9378 634.833 36.9378 632.9 Q36.9378 632.031 37.131 631.193 Q37.2921 630.324 37.7751 629.293 Q38.2582 628.23 39.321 627.296 Q40.3838 626.362 41.9297 625.783 Q41.189 625.396 40.4804 624.881 Q39.7719 624.333 38.9023 623.496 Q38.0006 622.627 37.4853 621.435 Q36.9378 620.211 36.9378 618.858 Q36.9378 617.538 37.2921 616.25 Q37.6141 614.962 38.6769 613.867 Q39.7075 612.739 41.2856 612.739 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip820)\" d=\"M81.9294 606.388 Q81.6073 606.388 80.8022 605.551 Q69.981 594.923 49.5947 594.923 Q29.144 594.923 18.8059 605.132 Q17.6143 606.388 17.26 606.388 Q16.6159 606.388 16.6159 605.744 Q16.6159 605.293 17.9363 603.715 Q19.2568 602.105 22.2841 599.689 Q25.3115 597.242 29.0151 595.438 Q38.0006 591.09 49.5947 591.09 Q60.7379 591.09 69.7234 595.245 Q73.4914 597.016 76.6476 599.496 Q79.8038 601.976 81.1886 603.618 Q82.5735 605.261 82.5735 605.744 Q82.5735 606.388 81.9294 606.388 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip822)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.067,1384.24 321.985,1384.23 441.43,1384.07 513.212,1383.84 573.903,1383.45 639.83,1382.65 700.609,1381.25 765.77,1378.44 841.266,1371.82 874.812,1366.87 \n",
       "  908.357,1360 937.774,1351.82 967.191,1340.98 983.227,1333.68 999.263,1325.2 1015.3,1315.38 1031.34,1304.04 1047.47,1290.89 1063.6,1275.78 1079.73,1258.47 \n",
       "  1095.86,1238.75 1104.78,1226.72 1113.7,1213.84 1122.61,1200.09 1131.53,1185.42 1140.45,1169.8 1149.37,1153.22 1158.28,1135.65 1167.2,1117.08 1175.69,1098.47 \n",
       "  1184.17,1078.95 1192.66,1058.53 1201.14,1037.22 1218.11,992.035 1235.08,943.694 1252.73,890.549 1270.37,835.099 1288.01,778.115 1305.66,720.459 1320.51,672.067 \n",
       "  1335.37,624.374 1350.22,577.883 1365.07,533.046 1374.14,506.656 1383.21,481.102 1392.27,456.445 1401.34,432.737 1410.41,410.017 1419.47,388.313 1428.54,367.643 \n",
       "  1437.61,348.015 1445.91,330.949 1454.22,314.749 1462.53,299.404 1470.83,284.898 1487.44,258.317 1504.05,234.81 1519.4,215.632 1534.74,198.685 1550.08,183.77 \n",
       "  1565.42,170.691 1580.96,159.119 1596.51,149.048 1612.05,140.305 1627.59,132.731 1645.01,125.455 1662.42,119.284 1679.84,114.06 1697.26,109.644 1728.82,103.32 \n",
       "  1760.39,98.6794 1832.67,92.215 1899.6,89.235 1965.58,87.727 2029.64,86.9662 2095.31,86.5599 2167.27,86.3365 2271.79,86.1999 2293.23,86.1857 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, LaTeXStrings\n",
    "\n",
    "function logistic(x)\n",
    "    return 1 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "plot(logistic, -10, 10, label=false,\n",
    "     xlabel=L\"x\", ylabel=L\"\\mathrm{Logistic}(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\fig{logistic}\n",
    "\\center{*Logistic Function*} \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the logistic function is basically a mapping of any real number to a\n",
    "real number in the range between 0 and 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Logistic}(x) = \\{ \\mathbb{R} \\in [- \\infty , + \\infty] \\} \\to \\{ \\mathbb{R} \\in (0, 1) \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the logistic function is the ideal candidate for when we need to convert something continuous without restrictions\n",
    "to something continuous restricted between 0 and 1. That is why it is used when we need a model to have a probability as a\n",
    "dependent variable (remembering that any real number between 0 and 1 is a valid probability). In the case of a binary dependent\n",
    "variable, we can use this probability as the chance of the dependent variable taking a value of 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression follows the following mathematical formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Linear} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\theta$ - model parameters\n",
    "  * $\\theta_0$ - intercept\n",
    "  * $\\theta_1, \\theta_2, \\dots$ - independent variables $x_1, x_2, \\dots$ coefficients\n",
    "* $n$ - total number of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression would add the logistic function to the linear term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\hat{p} = \\text{Logistic}(\\text{Linear}) = \\frac{1}{1 + e^{-\\operatorname{Linear}}}$ - predicted probability of the observation being the value 1\n",
    "* $\\hat{\\mathbf{y}}=\\left\\{\\begin{array}{ll} 0 & \\text { if } \\hat{p} < 0.5 \\\\ 1 & \\text { if } \\hat{p} \\geq 0.5 \\end{array}\\right.$ - predicted discrete value of $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Probability of Death} = \\text{Logistic} \\big(-10 + 10 \\cdot \\text{cancer} + 12 \\cdot \\text{diabetes} + 8 \\cdot \\text{obesity} \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model logistic regression in two ways. The first option with a **Bernoulli likelihood** function and the second option with\n",
    "a **binomial likelihood** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the **Bernoulli likelihood** we model a binary dependent variable $y$ which is the result of a Bernoulli trial with\n",
    "a certain probability $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **binomial likelihood**, we model a continuous dependent variable $y$ which is the number of successes of $n$\n",
    "independent Bernoulli trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bernoulli Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} &\\sim \\text{Bernoulli}\\left( p \\right) \\\\\n",
    "\\mathbf{p} &\\sim \\text{Logistic}(\\alpha + \\mathbf{X} \\cdot \\boldsymbol{\\beta}) \\\\\n",
    "\\alpha &\\sim \\text{Normal}(\\mu_\\alpha, \\sigma_\\alpha) \\\\\n",
    "\\boldsymbol{\\beta} &\\sim \\text{Normal}(\\mu_{\\boldsymbol{\\beta}}, \\sigma_{\\boldsymbol{\\beta}})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{y}$ -- binary dependent variable.\n",
    "* $\\mathbf{p}$ -- probability of $\\mathbf{y}$ taking the value of $\\mathbf{y}$ -- success of an independent Bernoulli trial.\n",
    "* $\\text{Logistic}$ -- logistic function.\n",
    "* $\\alpha$ -- intercept.\n",
    "* $\\boldsymbol{\\beta}$ -- coefficient vector.\n",
    "* $\\mathbf{X}$ -- data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Binomial Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} &\\sim \\text{Binomial}\\left( n, p \\right) \\\\\n",
    "\\mathbf{p} &\\sim \\text{Logistic}(\\alpha + \\mathbf{X} \\cdot \\boldsymbol{\\beta}) \\\\\n",
    "\\alpha &\\sim \\text{Normal}(\\mu_\\alpha, \\sigma_\\alpha) \\\\\n",
    "\\boldsymbol{\\beta} &\\sim \\text{Normal}(\\mu_{\\boldsymbol{\\beta}}, \\sigma_{\\boldsymbol{\\beta}})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{y}$ -- binary dependent variable -- successes of $n$ independent Bernoulli trials.\n",
    "* $n$ -- number of independent Bernoulli trials.\n",
    "* $\\mathbf{p}$ -- probability of $\\mathbf{y}$ taking the value of $\\mathbf{y}$ -- success of an independent Bernoulli trial.\n",
    "* $\\text{Logistic}$ -- logistic function.\n",
    "* $\\alpha$ -- intercept.\n",
    "* $\\boldsymbol{\\beta}$ -- coefficient vector.\n",
    "* $\\mathbf{X}$ -- data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both likelihood options, what remains is to specify the model parameters' prior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prior Distribution of $\\alpha$ -- Knowledge we possess regarding the model's intercept.\n",
    "* Prior Distribution of $\\boldsymbol{\\beta}$  -- Knowledge we possess regarding the model's independent variables' coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to instantiate a logistic regression with the observed data ($\\mathbf{y}$ and $\\mathbf{X}$) and find the posterior\n",
    "distribution of our model's parameters of interest ($\\alpha$ and $\\boldsymbol{\\beta}$). This means to find the full posterior\n",
    "distribution of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = P(\\alpha, \\boldsymbol{\\beta} \\mid \\mathbf{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that contrary to the linear regression, which used a Gaussian/normal likelihood function, we don't have an error\n",
    "parameter $\\sigma$ in our logistic regression. This is due to neither the Bernoulli nor binomial distributions having\n",
    "a \"scale\" parameter such as the $\\sigma$ parameter in the Gaussian/normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the Bernoulli distribution is a special case of the binomial distribution where $n = 1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Bernoulli}(p) = \\text{Binomial}(1, p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easily accomplished with Turing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: [Turing]: progress logging is disabled globally\n",
      "└ @ Turing /Users/svollmer/.julia/packages/Turing/rl6ku/src/Turing.jl:22\n",
      "┌ Info: [AdvancedVI]: global PROGRESS is set as false\n",
      "└ @ AdvancedVI /Users/svollmer/.julia/packages/AdvancedVI/W2zsz/src/AdvancedVI.jl:15\n"
     ]
    }
   ],
   "source": [
    "using Turing\n",
    "using LazyArrays\n",
    "using Random:seed!\n",
    "seed!(123)\n",
    "setprogress!(false) # hide\n",
    "\n",
    "@model function logreg(X,  y; predictors=size(X, 2))\n",
    "    #priors\n",
    "    α ~ Normal(0, 2.5)\n",
    "    β ~ filldist(TDist(3), predictors)\n",
    "\n",
    "    #likelihood\n",
    "    y ~ arraydist(LazyArray(@~ BernoulliLogit.(α .+ X * β)))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am specifying very weakly informative priors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha \\sim \\text{Normal}(0, 2.5)$ -- This means a normal distribution centered on 0 with a standard deviation of 2.5. That prior should with ease cover all possible values of $\\alpha$. Remember that the normal distribution has support over all the real number line $\\in (-\\infty, +\\infty)$.\n",
    "* $\\boldsymbol{\\beta} \\sim \\text{Student-}t(0,1,3)$ -- The predictors all have a prior distribution of a Student-$t$ distribution centered on 0 with variance 1 and degrees of freedom $\\nu = 3$. That wide-tailed $t$ distribution will cover all possible values for our coefficients. Remember the Student-$t$ also has support over all the real number line $\\in (-\\infty, +\\infty)$. Also the `filldist()` is a nice Turing's function which takes any univariate or multivariate distribution and returns another distribution that repeats the input distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turing's `arraydist()` function wraps an array of distributions returning a new distribution sampling from the individual\n",
    "distributions. And the LazyArrays' `LazyArray()` constructor wrap a lazy object that wraps a computation producing an array\n",
    "to an array. Last, but not least, the macro `@~` creates a broadcast and is a nice short hand for the familiar dot `.`\n",
    "broadcasting operator in Julia. This is an efficient way to tell Turing that our `y` vector is distributed lazily as a\n",
    "`BernoulliLogit` broadcasted to `α` added to the product of the data matrix `X` and `β` coefficient vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dependent variable `y` is continuous and represents the number of successes of $n$ independent Bernoulli trials\n",
    "you can use the binomial likelihood in your model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "y ~ arraydist(LazyArray(@~ BinomialLogit.(n, α .+ X * β)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Contaminated Water Wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, I will use a famous dataset called `wells` (Gelman & Hill, 2007), which is data from a survey of 3,200\n",
    "residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated\n",
    "arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby\n",
    "area and the survey was conducted several years later to learn which of the affected residents had switched wells.\n",
    "It has 3,200 observations and the following variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `switch` -- binary/dummy (0 or 1) for well-switching.\n",
    "* `arsenic` -- arsenic level in respondent's well.\n",
    "* `dist` -- distance (meters) from the respondent's house to the nearest well with safe drinking water.\n",
    "* `association` -- binary/dummy (0 or 1) if member(s) of household participate in community organizations.\n",
    "* `educ` -- years of education (head of household)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's read our data with `CSV.jl` and output into a `DataFrame` from `DataFrames.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Float64\">Float64</th><th title=\"Real\">Real</th><th title=\"Float64\">Float64</th><th title=\"Real\">Real</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>switch</td><td>0.575166</td><td>0</td><td>1.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>arsenic</td><td>1.65693</td><td>0.51</td><td>1.3</td><td>9.65</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>dist</td><td>48.3319</td><td>0.387</td><td>36.7615</td><td>339.531</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>assoc</td><td>0.422848</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>5</th><td>educ</td><td>4.82848</td><td>0</td><td>5.0</td><td>17</td><td>0</td><td>Int64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Real & Float64 & Real & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & switch & 0.575166 & 0 & 1.0 & 1 & 0 & Int64 \\\\\n",
       "\t2 & arsenic & 1.65693 & 0.51 & 1.3 & 9.65 & 0 & Float64 \\\\\n",
       "\t3 & dist & 48.3319 & 0.387 & 36.7615 & 339.531 & 0 & Float64 \\\\\n",
       "\t4 & assoc & 0.422848 & 0 & 0.0 & 1 & 0 & Int64 \\\\\n",
       "\t5 & educ & 4.82848 & 0 & 5.0 & 17 & 0 & Int64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m min   \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max     \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Real  \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Real    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │ switch     0.575166  0       1.0       1             0  Int64\n",
       "   2 │ arsenic    1.65693   0.51    1.3       9.65          0  Float64\n",
       "   3 │ dist      48.3319    0.387  36.7615  339.531         0  Float64\n",
       "   4 │ assoc      0.422848  0       0.0       1             0  Int64\n",
       "   5 │ educ       4.82848   0       5.0      17             0  Int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames, CSV, HTTP\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/storopoli/Bayesian-Julia/master/datasets/wells.csv\"\n",
    "wells = CSV.read(HTTP.get(url).body, DataFrame)\n",
    "describe(wells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the `describe()` output 58% of the respondents switched wells and 42% percent of respondents\n",
    "somehow are engaged in community organizations. The average years of education of the household's head is approximate\n",
    "5 years and ranges from 0 (no education at all) to 17 years. The distance to safe drinking water is measured in meters\n",
    "and averages 48m ranging from less than 1m to 339m. Regarding arsenic levels I cannot comment because the only thing I\n",
    "know that it is toxic and you probably would never want to have your well contaminated with it. Here, we believe that all\n",
    "of those variables somehow influence the probability of a respondent switch to a safe well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's us instantiate our model with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Matrix(select(wells, Not(:switch)))\n",
    "y = wells[:, :switch]\n",
    "model = logreg(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we will sample from the Turing model. We will be using the default `NUTS()` sampler with `2_000` samples, with\n",
    "4 Markov chains using multiple threads `MCMCThreads()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n",
      "└ @ AbstractMCMC /Users/svollmer/.julia/packages/AbstractMCMC/6aLyN/src/sample.jl:292\n",
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.00625\n",
      "└ @ Turing.Inference /Users/svollmer/.julia/packages/Turing/rl6ku/src/inference/hmc.jl:188\n",
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.003125\n",
      "└ @ Turing.Inference /Users/svollmer/.julia/packages/Turing/rl6ku/src/inference/hmc.jl:188\n",
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.003125\n",
      "└ @ Turing.Inference /Users/svollmer/.julia/packages/Turing/rl6ku/src/inference/hmc.jl:188\n",
      "┌ Info: Found initial step size\n",
      "│   ϵ = 0.00625\n",
      "└ @ Turing.Inference /Users/svollmer/.julia/packages/Turing/rl6ku/src/inference/hmc.jl:188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Summary Statistics\n",
       " \u001b[1m parameters \u001b[0m \u001b[1m    mean \u001b[0m \u001b[1m     std \u001b[0m \u001b[1m naive_se \u001b[0m \u001b[1m    mcse \u001b[0m \u001b[1m       ess \u001b[0m \u001b[1m    rhat \u001b[0m \u001b[1m \u001b[0m ⋯\n",
       " \u001b[90m     Symbol \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m \u001b[0m ⋯\n",
       "\n",
       "           α   -0.1540    0.0996     0.0011    0.0016   3545.5946    1.0016    ⋯\n",
       "        β[1]    0.4667    0.0418     0.0005    0.0006   4416.7552    1.0021    ⋯\n",
       "        β[2]   -0.0090    0.0011     0.0000    0.0000   9316.8773    1.0004    ⋯\n",
       "        β[3]   -0.1240    0.0770     0.0009    0.0011   5245.9654    1.0012    ⋯\n",
       "        β[4]    0.0424    0.0095     0.0001    0.0001   5884.9551    1.0000    ⋯\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = sample(model, NUTS(), MCMCThreads(), 2_000, 4)\n",
    "summarystats(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had no problem with the Markov chains as all the `rhat` are well below `1.01` (or above `0.99`).\n",
    "Note that the coefficients are in log-odds scale. They are the natural log of the odds[^logit], and odds is\n",
    "defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{odds} = \\frac{p}{1-p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p$ is a probability. So log-odds is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log(\\text{odds}) = \\log \\left( \\frac{p}{1-x} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to get odds from a log-odds we must undo the log operation with a exponentiation.\n",
    "This translates to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{odds} = \\exp ( \\log ( \\text{odds} )) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with a transformation in a `DataFrame` constructed from a `Chains` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 6 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>parameters</th><th>2.5%</th><th>25.0%</th><th>50.0%</th><th>75.0%</th><th>97.5%</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>α</td><td>0.706142</td><td>0.802427</td><td>0.857271</td><td>0.915824</td><td>1.04359</td></tr><tr><th>2</th><td>β[1]</td><td>1.47215</td><td>1.54942</td><td>1.59496</td><td>1.64084</td><td>1.73016</td></tr><tr><th>3</th><td>β[2]</td><td>0.988962</td><td>0.990332</td><td>0.991061</td><td>0.991751</td><td>0.993129</td></tr><tr><th>4</th><td>β[3]</td><td>0.758161</td><td>0.838874</td><td>0.883846</td><td>0.930696</td><td>1.02433</td></tr><tr><th>5</th><td>β[4]</td><td>1.02396</td><td>1.03668</td><td>1.04331</td><td>1.04999</td><td>1.06322</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& parameters & 2.5\\% & 25.0\\% & 50.0\\% & 75.0\\% & 97.5\\%\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & α & 0.706142 & 0.802427 & 0.857271 & 0.915824 & 1.04359 \\\\\n",
       "\t2 & β[1] & 1.47215 & 1.54942 & 1.59496 & 1.64084 & 1.73016 \\\\\n",
       "\t3 & β[2] & 0.988962 & 0.990332 & 0.991061 & 0.991751 & 0.993129 \\\\\n",
       "\t4 & β[3] & 0.758161 & 0.838874 & 0.883846 & 0.930696 & 1.02433 \\\\\n",
       "\t5 & β[4] & 1.02396 & 1.03668 & 1.04331 & 1.04999 & 1.06322 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m parameters \u001b[0m\u001b[1m 2.5%     \u001b[0m\u001b[1m 25.0%    \u001b[0m\u001b[1m 50.0%    \u001b[0m\u001b[1m 75.0%    \u001b[0m\u001b[1m 97.5%    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol     \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────\n",
       "   1 │ α           0.706142  0.802427  0.857271  0.915824  1.04359\n",
       "   2 │ β[1]        1.47215   1.54942   1.59496   1.64084   1.73016\n",
       "   3 │ β[2]        0.988962  0.990332  0.991061  0.991751  0.993129\n",
       "   4 │ β[3]        0.758161  0.838874  0.883846  0.930696  1.02433\n",
       "   5 │ β[4]        1.02396   1.03668   1.04331   1.04999   1.06322"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Chain\n",
    "\n",
    "@chain quantile(chain) begin\n",
    "    DataFrame\n",
    "    select(_,\n",
    "        :parameters,\n",
    "        names(_, r\"%\") .=> ByRow(exp),\n",
    "        renamecols=false)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interpretation of odds is the same as in betting games. Anything below 1 signals a unlikely probability that $y$ will be $1$.\n",
    "And anything above 1 increases the probability of $y$ being $1$, while 1 itself is a neutral odds for $y$ being either $1$ or $0$.\n",
    "Since I am not a gambling man, let's talk about probabilities. So I will create a function called `logodds2prob()` that converts\n",
    "log-odds to probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 6 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>parameters</th><th>2.5%</th><th>25.0%</th><th>50.0%</th><th>75.0%</th><th>97.5%</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>α</td><td>0.413882</td><td>0.445193</td><td>0.461576</td><td>0.478031</td><td>0.510665</td></tr><tr><th>2</th><td>β[1]</td><td>0.595493</td><td>0.607754</td><td>0.614637</td><td>0.621332</td><td>0.633722</td></tr><tr><th>3</th><td>β[2]</td><td>0.497225</td><td>0.497571</td><td>0.497755</td><td>0.497929</td><td>0.498276</td></tr><tr><th>4</th><td>β[3]</td><td>0.431224</td><td>0.456189</td><td>0.469171</td><td>0.482052</td><td>0.50601</td></tr><tr><th>5</th><td>β[4]</td><td>0.505918</td><td>0.509004</td><td>0.510597</td><td>0.512193</td><td>0.515321</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& parameters & 2.5\\% & 25.0\\% & 50.0\\% & 75.0\\% & 97.5\\%\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & α & 0.413882 & 0.445193 & 0.461576 & 0.478031 & 0.510665 \\\\\n",
       "\t2 & β[1] & 0.595493 & 0.607754 & 0.614637 & 0.621332 & 0.633722 \\\\\n",
       "\t3 & β[2] & 0.497225 & 0.497571 & 0.497755 & 0.497929 & 0.498276 \\\\\n",
       "\t4 & β[3] & 0.431224 & 0.456189 & 0.469171 & 0.482052 & 0.50601 \\\\\n",
       "\t5 & β[4] & 0.505918 & 0.509004 & 0.510597 & 0.512193 & 0.515321 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m parameters \u001b[0m\u001b[1m 2.5%     \u001b[0m\u001b[1m 25.0%    \u001b[0m\u001b[1m 50.0%    \u001b[0m\u001b[1m 75.0%    \u001b[0m\u001b[1m 97.5%    \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol     \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────\n",
       "   1 │ α           0.413882  0.445193  0.461576  0.478031  0.510665\n",
       "   2 │ β[1]        0.595493  0.607754  0.614637  0.621332  0.633722\n",
       "   3 │ β[2]        0.497225  0.497571  0.497755  0.497929  0.498276\n",
       "   4 │ β[3]        0.431224  0.456189  0.469171  0.482052  0.50601\n",
       "   5 │ β[4]        0.505918  0.509004  0.510597  0.512193  0.515321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function logodds2prob(logodds::Float64)\n",
    "    return exp(logodds) / (1 + exp(logodds))\n",
    "end\n",
    "\n",
    "@chain quantile(chain) begin\n",
    "    DataFrame\n",
    "    select(_,\n",
    "        :parameters,\n",
    "        names(_, r\"%\") .=> ByRow(logodds2prob),\n",
    "        renamecols=false)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, much better now. Let's analyze our results. The intercept `α` is the basal `switch` probability which has\n",
    "a median value of 46%. All coefficients whose 95% credible intervals captures the value $\\frac{1}{2} = 0.5$ tells\n",
    "that the effect on the propensity of `switch` is inconclusive. It is pretty much similar to a 95% credible interval\n",
    "that captures the 0 in the linear regression coefficients. So this rules out `β[3]` which is the third column of `X`\n",
    "-- `assoc`. The other remaining 95% credible intervals can be interpreted as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `β[1]` -- first column of `X`, `arsenic`, has 95% credible interval 0.595 to 0.634. This means that each increase in one unit of `arsenic` is related to an increase of 9.6% to 13.4% propension of `switch` being 1.\n",
    "* `β[2]` -- second column of `X`, `dist`, has a 95% credible interval from 0.497 to 0.498. So we expect that each increase in one meter of `dist` is related to a decrease of 0.1% propension of `switch` being 0.\n",
    "* `β[4]` -- fourth column of `X`, `educ`, has a 95% credible interval from 0.506 to 0.515. Each increase in one year of `educ` is related to an increase of 0.6% to 1.5% propension of `switch` being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how you interpret 95% credible intervals from a `quantile()` output of a logistic regression `Chains`\n",
    "object converted from log-odds to probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^logit]: actually the [logit](https://en.wikipedia.org/wiki/Logit) function or the log-odds is the logarithm of the odds $\\frac{p}{1-p}$ where $p$ is a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge university press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
